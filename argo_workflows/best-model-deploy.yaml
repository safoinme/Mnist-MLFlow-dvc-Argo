apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: mnist-deploy-workflow
spec:
  entrypoint: mnist-workflow-best-model-deploy
  volumes:
  - name: saved-model-artifact
  templates:
  - name: mnist-workflow-best-model-deploy
    steps:
    - - name: retrieve-best-model           # retrieve-best-model is run before the following steps and after training steps
        template: mlflow-mnist-best-model
    - - name: deploy-model           # deploy-model is run after the retrieve-best-model step and only if retrieve-best-model have tf result
        template: mnist-deploy-tf-serving
        when: "{{steps.retrieve-best-model.outputs.parameters.model-flavor}} == tf"
        arguments:
          parameters:
            - name: predictor
              value: "tensorflow"
            - name: model-uri
              value: "{{steps.retrieve-best-model.outputs.parameters.model-uri}}"

  - name: mlflow-mnist-best-model
    inputs:
      artifacts:
      - name: repo-source
        path: /app
        git:
          repo: https://github.com/safoinme/Mnist-MLFlow-dvc-Argo.git
          revision: "argo"
    container:
      image: safoinme/python-mlflow-expirements:0.1.0
      env:
          - name: MLFLOW_TRACKING_URI
            valueFrom:
              secretKeyRef:
                name: mlflow-access
                key: MLFLOW_TRACKING_URI
          - name: MLFLOW_S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: mlflow-access
                key: MLFLOW_S3_ENDPOINT_URL
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: mlflow-access
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: mlflow-access
                key: AWS_SECRET_ACCESS_KEY
      command: [sh, -c]
      args: 
        - python3 experiment_comparaison.py;      
      workingDir: /app
    outputs:
      parameters:
        - name: model-flavor
          valueFrom:
            path: /tmp/artifact_downloads/model/data/save_format.txt
        - name: model-uri
          valueFrom:
            path: /tmp/best_model_artifact_uri.txt
  #- name: mnist-deploy-tf-serving
  #  inputs:
  #    parameters:
  #    - name: model-uri
  #  container:
  #    image: safoinme/python-mlflow-expirements:0.1.0
  #    command: [sh, -c]
  #    args: ["echo {{inputs.parameters.model-uri}} "]  
  
  - name: mnist-deploy-tf-serving
    inputs:
      parameters:
      - name: model-uri
      - name: predictor
    resource:
      action: create
      manifest: |
        apiVersion: "serving.kserve.io/v1beta1"
        kind: "InferenceService"
        metadata:
          name: "mnist-example-s3"
        spec:
          predictor:
            serviceAccountName: stminio
            {{inputs.parameters.predictor}}:
              storageUri: "{{inputs.parameters.model-uri}}"
              resources:
                limits:
                  memory: 2Gi
                  cpu: "1"